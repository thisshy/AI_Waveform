# BNS Waveform Generator Based on Conditional Autoencoders

This repository provides the source code for the paper:

**"Conditional Autoencoder for Generating Binary Neutron Star Waveforms with Tidal and Precession Effects"**

The project introduces a Conditional Autoencoder (cAE) for efficient and accurate generation of gravitational waveforms from Binary Neutron Star (BNS) mergers. It supports waveform reconstruction conditioned on physical parameters such as component masses, spins, and tidal deformabilities, and achieves high precision while maintaining strong generation efficiency.


##  Environment Setup

This project is developed based on **Python 3.10** and **TensorFlow 2.15**. To ensure compatibility, we recommend setting up a dedicated virtual environment.

###  Required Dependencies

The key dependencies include:

- `tensorflow==2.15.0`
- `keras-tuner`
- `numpy`
- `matplotlib`
- `scikit-learn`
- `pycbc`
- `seaborn`
- `tqdm`

All required packages are listed in `requirements.txt`.

###  Installation Instructions

You can install the environment using the following steps:

```bash
# Create a virtual environment (recommended)
conda create -n bns-waveform python=3.10 -y
conda activate bns-waveform

# Install dependencies from requirements.txt
pip install -r requirements.txt
```



## Model Architecture

The figure below illustrates the architecture of the proposed Conditional Autoencoder (cAE) model for gravitational waveform generation. The model consists of three main components: (1) the autoencoder for waveform reconstruction, (2) the conditional encoder that maps physical parameters (e.g., masses, spins, tidal deformabilities) to latent space, and (3) the shared decoder used during inference to generate waveforms from encoded conditions. The encoder and decoder modules integrate convolutional layers, ResNet blocks, and Transformer blocks for temporal feature extraction and reconstruction.

![Model Architecture](./total_network.png)

*Figure: Architecture of the cAE model used for training and inference. The training stage includes both waveform autoencoding and latent supervision via physical parameters. At test time, only the condition encoder and decoder are used to generate waveforms from parameter input.*

## Model Training and Reproduction

This repository provides scripts to reproduce both the Conditional Autoencoder (cAE) and the Residual-MLP-CNN baseline models described in the paper. All training code is located in the `/models/` directory, and preprocessed datasets are expected under `/data/`.

###  Training Scripts

- `cAE_amplitude.py`: trains the conditional autoencoder model for waveform amplitude reconstruction.
- `cAE_phase.py`: trains the conditional autoencoder model for waveform phase reconstruction.
- `res_mlp_cnn_amplitude.py`: trains the Residual-MLP-CNN baseline for amplitude.
- `res_mlp_cnn_phase.py`: trains the Residual-MLP-CNN baseline for phase.

###  How to Train

To train the models, run the scripts as follows:

```bash
# Train cAE models
python models/cAE_amplitude.py
python models/cAE_phase.py

# Train Res-MLP-CNN models
python models/res_mlp_cnn_amplitude.py
python models/res_mlp_cnn_phase.py
```

##  Inference, Evaluation, and Analysis

Once the models in the `/models/` directory have been successfully trained, you can generate waveforms directly using:

- `example.ipynb`: demonstrates how to load the trained cAE models and predict waveform polarizations (hp, hc) from a given parameter vector. This is the recommended entry point for waveform inference.

Other scripts in this directory are provided for reproducing the key evaluation results reported in the paper:

### üîÅ Waveform Generation
- `Generate_waveform.py`: a reusable module to load models and generate waveforms given physical parameters (used internally by other notebooks/scripts).

### üìà Precision Comparison
- `cAE_mismatch.py`: computes mismatch statistics of the cAE-generated waveforms.
- `res_mlp_cnn_mismatch.py`: computes mismatch for the Res-MLP-CNN baseline.

### ‚è±Ô∏è Timing Benchmark
- `time_cAE_cpu.py` / `time_cAE_gpu.py`: measure waveform generation time for the cAE model on CPU and GPU respectively.
- `time_res_mlp_cnn_cpu.py` / `time_res_mlp_cnn_gpu.py`: similar timing benchmarks for the Res-MLP-CNN model.
- `time_pycbc.py`: serves as a baseline timing comparison for waveforms generated by PyCBC.

### üìä Visualization
- `Visualization.ipynb`: plots mismatch histograms, evaluation summaries used in the final paper figures.




---
##  Repository Structure

```bash
BNS Waveform Generator Based on Conditional Autoencoders/
‚îú‚îÄ‚îÄ /data/                  # Waveform generation and construction of dataset 
‚îú‚îÄ‚îÄ /models/                # Model architectures and training steps for cAE and Res-MLP-CNN  
‚îú‚îÄ‚îÄ /results_and_analysis/  # Code for reproducing figures in the paper and usage examples of the models  
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt        # Environment dependencies
```

